{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.2: Naive Bayes implementation from scratch\n",
    "Now that you have understood the ins and outs of Bayes Theorem, we will extend it to consider cases where we have more than one feature.\n",
    "\n",
    "Let's say that we have two political parties' candidates, 'Jill Stein' of the Green Party and 'Gary Johnson' of the Libertarian Party and we have the probabilities of each of these candidates saying the words 'freedom', 'immigration' and 'environment' when they give a speech:\n",
    "\n",
    "Probability that Jill Stein says 'freedom': 0.1 ---------> P(F|J)\n",
    "Probability that Jill Stein says 'immigration': 0.1 -----> P(I|J)\n",
    "Probability that Jill Stein says 'environment': 0.8 -----> P(E|J)\n",
    "Probability that Gary Johnson says 'freedom': 0.7 -------> P(F|G)\n",
    "Probability that Gary Johnson says 'immigration': 0.2 ---> P(I|G)\n",
    "Probability that Gary Johnson says 'environment': 0.1 ---> P(E|G)\n",
    "And let us also assume that the probability of Jill Stein giving a speech, P(J) is 0.5 and the same for Gary Johnson, P(G) = 0.5.\n",
    "\n",
    "Given this, what if we had to find the probabilities of Jill Stein saying the words 'freedom' and 'immigration'? This is where the Naive Bayes' theorem comes into play as we are considering two features, 'freedom' and 'immigration'.\n",
    "\n",
    "Now we are at a place where we can define the formula for the Naive Bayes' theorem:\n",
    "\n",
    "\n",
    "Here, y is the class variable (in our case the name of the candidate) and x1 through xn are the feature vectors (in our case the individual words). The theorem makes the assumption that each of the feature vectors or words (xi) are independent of each other.\n",
    "\n",
    "To break this down, we have to compute the following posterior probabilities:\n",
    "\n",
    "P(J|F,I): Given the words 'freedom' and 'immigration' were said, what's the probability they were said by Jill?\n",
    "\n",
    "Using the formula and our knowledge of Bayes' theorem, we can compute this as follows: P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I). Here P(F,I) is the probability of the words 'freedom' and 'immigration' being said in a speech.\n",
    "\n",
    "P(G|F,I): Given the words 'freedom' and 'immigration' were said, what's the probability they were said by Gary?\n",
    "\n",
    "Using the formula, we can compute this as follows: P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010000000000000002\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
    "P(F,I).\n",
    "\n",
    "The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text.\n",
    "\n",
    "The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text.\n",
    "\n",
    "The third step is to add both of these probabilities and you will get P(F,I).\n",
    "'''\n",
    "# p_j_text\n",
    "'''\n",
    "Solution: Step 1\n",
    "'''\n",
    "# P(J)\n",
    "p_j = 0.5\n",
    "\n",
    "# P(F/J)\n",
    "p_j_f = 0.1\n",
    "\n",
    "# P(I/J)\n",
    "p_j_i = 0.1\n",
    "\n",
    "p_j_text = p_j_f * p_j_i\n",
    "print(p_j_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13999999999999999\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution: Step 2\n",
    "'''\n",
    "# P(G)\n",
    "p_g = 0.5\n",
    "\n",
    "# P(F/G)\n",
    "p_g_f = 0.7\n",
    "\n",
    "# P(I/G)\n",
    "p_g_i = 0.2\n",
    "\n",
    "p_g_text = p_g_f * p_g_i\n",
    "print(p_g_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words freedom and immigration being said are:  0.075\n",
      "The probability of Jill Stein saying the words Freedom and Immigration:  0.06666666666666668\n",
      "The probability of Gary Johnson saying the words Freedom and Immigration:  0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution: Step 3: Compute P(F,I) and store in p_f_i\n",
    "'''\n",
    "p_f_i = p_j * p_j_text + p_g * p_g_text\n",
    "print('Probability of words freedom and immigration being said are: ', format(p_f_i))\n",
    "# Now we can compute the probability of P(J|F,I), the probability of Jill Stein saying the words 'freedom' and 'immigration' and P(G|F,I), the probability of Gary Johnson saying the words 'freedom' and 'immigration'.\n",
    "\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\n",
    "'''\n",
    "'''\n",
    "Solution\n",
    "'''\n",
    "p_j_fi = (p_j * p_j_f * p_j_i) / p_f_i\n",
    "print('The probability of Jill Stein saying the words Freedom and Immigration: ', format(p_j_fi))\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\n",
    "'''\n",
    "'''\n",
    "Solution\n",
    "'''\n",
    "p_g_fi = (p_g * p_g_f * p_g_i) / p_f_i\n",
    "print('The probability of Gary Johnson saying the words Freedom and Immigration: ', format(p_g_fi))\n",
    "# And as we can see, just like in the Bayes' theorem case, the sum of our posteriors is equal to 1.\n",
    "\n",
    "# Congratulations! You have implemented the Naive Bayes' theorem from scratch. Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words 'freedom' and 'immigration' in her speech as compared with the 93.3% chance for Gary Johnson of the Libertarian party.\n",
    "\n",
    "# For another example of Naive Bayes, let's consider searching for images using the term 'Sacramento Kings' in a search engine. In order for us to get the results pertaining to the Scramento Kings NBA basketball team, the search engine needs to be able to associate the two words together and not treat them individually. If the search engine only searched for the words individually, we would get results of images tagged with 'Sacramento,' like pictures of city landscapes, and images of 'Kings,' which might be pictures of crowns or kings from history. But associating the two terms together would produce images of the basketball team. In the first approach we would treat the words as independent entities, so it would be considered 'naive.' We don't usually want this approach from a search engine, but it can be extremely useful in other cases.\n",
    "\n",
    "# Applying this to our problem of classifying messages as spam, the Naive Bayes algorithm looks at each word individually and not as associated entities with any kind of link between them. In the case of spam detectors, this usually works, as there are certain red flag words in an email which are highly reliable in classifying it as spam. For example, emails with words like 'viagra' are usually classified as spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented the Naive Bayes' theorem from scratch. Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words 'freedom' and 'immigration' in her speech as compared with the 93.3% chance for Gary Johnson of the Libertarian party.\n",
    "\n",
    " For another example of Naive Bayes, let's consider searching for images using the term 'Sacramento Kings' in a search engine. In order for us to get the results pertaining to the Scramento Kings NBA basketball team, the search engine needs to be able to associate the two words together and not treat them individually. If the search engine only searched for the words individually, we would get results of images tagged with 'Sacramento,' like pictures of city landscapes, and images of 'Kings,' which might be pictures of crowns or kings from history. But associating the two terms together would produce images of the basketball team. In the first approach we would treat the words as independent entities, so it would be considered 'naive.' We don't usually want this approach from a search engine, but it can be extremely useful in other cases.\n",
    "\n",
    " Applying this to our problem of classifying messages as spam, the Naive Bayes algorithm looks at each word individually and not as associated entities with any kind of link between them. In the case of spam detectors, this usually works, as there are certain red flag words in an email which are highly reliable in classifying it as spam. For example, emails with words like 'viagra' are usually classified as spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
